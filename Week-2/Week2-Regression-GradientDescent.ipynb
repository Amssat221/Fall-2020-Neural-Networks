{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problems\n",
    "\n",
    "**Regression problems** are problems where we try to predict a numerical output value given some input datapoint based on some examples of input datapoints that have known outputs.\n",
    "\n",
    "## Examples\n",
    "\n",
    "1. If we have a dataset of workers in a certain position of numbers of years of experience as an input and salary, we might want to predict salaries we don't know based on the years of experience a person has.\n",
    "\n",
    "1. If we have a dataset of variables about houses (floorspace, number of bedrooms, number of bathrooms, number of stories, age of the house) along with their selling prices, we might want to predict selling prices of homes not in the dataset based on the other variables about the house.\n",
    "\n",
    "1. If we have a dataset of variables about countries (average salary, average education of citizens, death rate, birth rate, infant mortality rate, etc.) along with their GDP, we might want to predict the GDP of countires not in the dataset based on the other variables about the country.\n",
    "\n",
    "1. If we have a dataset of seasonal variables about NBA basketball teams (points per game, turnovers per game, point differential, rebounds per game, blocks per game, etc.) and the numbers of wins they had in different seasons, we might want to take the statistics of a team early in a season to try to predict the number of wins they will have.\n",
    "\n",
    "## Types of Regression Problems\n",
    "\n",
    "There are different types of regression problems based on the numbers of input variables and output variables.\n",
    "\n",
    "* A **simple** regression problem predicts an output variable with just one input variable like Example 1.\n",
    "\n",
    "* A **multiple** regression problem predicts an output variable with more than one input variable like Examples 2 and 3.\n",
    "\n",
    "* A **multivariate** regression problem predicts more than one output variables\n",
    "\n",
    "We will not discuss multivariate regression this week, but we will show how neural networks can be used for these problems in the future.\n",
    "\n",
    "## The Math of a Regression Problem\n",
    "\n",
    "All of these regression problems have some things in common: there are example datapoints with outputs and we want to predict the outputs for new datapoints. Consider a $d$-dimensional point, or vector, $x_1\\in\\mathbb{R}^d$ and denote $x_1=(x_{11},x_{12},...,x_{1d})$. $x_1$ maps to an output $y_1\\in\\mathbb{R}$. We call the point $x_1$ an **example** and we call $y_1$ the **output** of $x_1$. (In statistics, the components of the vector $x_1$ are more frequently called **predictors** or **independent variables** and the $y_i$ values are more frequently called the **response** or **dependent variable**.)\n",
    "\n",
    "In a perfect world, a solution to a (univariate) regression problem will find a function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that can do two things:\n",
    "\n",
    "1. Mapping each example $x_i$ in a dataset to its output $y_i=f(x_i)$\n",
    "1. Generalize to successfully predict outputs of new datapoints\n",
    "\n",
    "In reality, $f$ will not always map each input $x_i$ values to each $y_i$ value perfectly or generalize to new inputs perfectly, but we try to get as close to this ideal as possible.\n",
    "\n",
    "## Regression Algorithms\n",
    "\n",
    "There are a number of popular approaches to regression problems, including the following.\n",
    "\n",
    "* least squares regression\n",
    "* lasso regression\n",
    "* ridge regression\n",
    "* support vector machines\n",
    "* neural networks\n",
    "\n",
    "This week, we will consider the first three approaches as they are quite similar. As an added bonus, they all use a numerical optimization scheme called stochastic gradient descent, which is one of the main algorithms needed to make neural networks work.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "We will assume the function $f$ we aim to predict is linear in some parameters $\\beta_1,...,\\beta_d$ so that our predicted function will be\n",
    "\n",
    "$$\n",
    "f(x_i)=\\sum\\limits_{k=1}^d \\beta_k x_{ik}=\\beta_1 x_{i1}+\\cdots+\\beta_d x_{id}\n",
    "$$\n",
    "\n",
    "and we will try to choose $\\beta_1,...,\\beta_d$ that will minimize a loss function on a training dataset $(x_1,y_1),...,(x_n,y_n)$. This loss function will be small if each $f(x_i)$ is near each $y_i$, which is what we want\n",
    "\n",
    "### Note\n",
    "\n",
    "It is a common misconception that \"linear regression\" must fit linear functions to data, but the \"linear\" part of linear regression refers to the fact that $f$ is linear with respect to $\\beta_1,...,\\beta_d$, not with respect to $x_i$, so it is certainly possible to apply some preprocessing to the datapoints.\n",
    "\n",
    "For example, if each $x_i\\in\\mathbb{R}^1$, we can fit a parabola by manipulating each $x_i$ into $x_i^*=(1,x_i,x_i^2)$ so that our predicted function would be\n",
    "\n",
    "$$f(x_i)=\\beta_1+\\beta_2x_i+\\beta_3x_i^2.$$\n",
    "\n",
    "While we will discuss only $x=(x_1,...,x_d)$, keep in mind that we can always create new variables from the data, preprocess the data into different forms, and so on. This topics go beyond the scope of this course, but I want you to realize linear regression can learn to represent functions far beyond simply lines and planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression by Ordinary Least Squares\n",
    "\n",
    "Recall that we have a labeled dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with each $x_i\\in\\mathbb{R}^d$ and each $y_i\\in\\mathbb{R}$ and our goal is to find a function $f$ that maps the $x_i$'s to the $y_i$'s as well as possible, and, we hope, effective at mapping unknown datapoints to appropriate outputs.\n",
    "\n",
    "In the **ordinary least squares** method, we try to minimize a the **sum of squared differences** between the real outputs $y_1, ..., y_n$ and the predicted outputs $f(x_1)$, ..., $f(x_n)$. In other words, the loss function in this method is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})&=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which we will call a **loss function**, where\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=\n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\\mathbf{y}=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\\mathbf{\\beta}=\\begin{pmatrix}\n",
    "\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Given this, we can rewrite the loss function as\n",
    "\n",
    "$$L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})=\\|\\mathbf{X}\\mathbf{\\beta}-\\mathbf{y}\\|^2_2=(\\mathbf{X}\\mathbf{\\beta}-\\mathbf{y})^T(\\mathbf{X}\\mathbf{\\beta}-\\mathbf{y}),$$\n",
    "\n",
    "where the $T$ subscript represents the **transpose** of a matrix. Here, $\\mathbf{X}$ and $\\mathbf{y}$ are constants derived from the dataset, so the $\\mathbf{\\beta}$ values are the only unknowns, so we need to find the $\\mathbf{\\beta}$ values that minimize the loss function $L$.\n",
    "\n",
    "In multivariate calculus, the approach to minimize a differentiable function is to take derivatives with respect to $\\beta_0, ..., \\beta_d$, set them all equal to zero, and solve. This may seem difficult, but we can actually do it for ordinary least squares. Before we take derivatives, let's convert the los function into a (longer but) simpler form. Transposes can be applied to sums of matrices separately, so\n",
    "\n",
    "$$L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})=((\\mathbf{X\\beta})^T-\\mathbf{y}^T)(\\mathbf{X}\\mathbf{\\beta}-\\mathbf{y}).$$\n",
    "\n",
    "Using the distributive property of matrix multiplication,\n",
    "\n",
    "$$L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})=(\\mathbf{X\\beta})^T\\mathbf{X\\beta}-(\\mathbf{X\\beta})^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{X\\beta}+\\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "If we realize $y$ and $\\mathbf{X}\\mathbf{\\beta}$ are both matrices of shape $n\\times 1$, then we should have $(\\mathbf{X\\beta})^T\\mathbf{y}=\\mathbf{y}^T\\mathbf{X\\beta}$, so the loss function is\n",
    "\n",
    "$$L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})=(\\mathbf{X\\beta})^T\\mathbf{X\\beta}-2(\\mathbf{X\\beta})^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "Since $(AB)^T=B^TA^T$ for matrices, we can simplify the terms as\n",
    "\n",
    "$$L(\\mathbf{X},\\mathbf{y},\\mathbf{\\beta})=\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X\\beta}-2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "Now, of course, this is a scalar (because, in the end, the loss is just a number--the sum of squared errors), so we can take the derivatives with respect to $\\beta_1$, ..., $\\beta_d$ and put those into a vector as\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{\\beta}}=2\\mathbf{X}^T\\mathbf{X\\beta}-2\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "As in multivariate calculus, we set this whole vector equal to 0 and solve for the estimated version of $\\mathbf{\\beta}$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2\\mathbf{X}^T\\mathbf{X\\beta}-2\\mathbf{X}^T\\mathbf{y} &= 0\\\\\n",
    "\\mathbf{X}^T\\mathbf{X\\beta} &= \\mathbf{X}^T\\mathbf{y}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, if $\\mathbf{X}^T\\mathbf{X}$ is an invertible matrix, then we can multiply both sides of the equation by its inverse to solve for $\\mathbf{\\beta}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X})\\mathbf{\\beta} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\\n",
    "\\mathbf{\\beta}&=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This does not look so nice to do by hand since it requires a matrix multiplication, a matrix inverse, and two more matrix multiplications, but a computer can complete these tasks quickly. (The best algorithms for matrix multiplication and matrix inverses for $n\\times n$ matrices each have computational complexity less than $O(n^3)$, so doing a few of these is no problem, even for quite large matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: High School Graduation Rates in US States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares by Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
